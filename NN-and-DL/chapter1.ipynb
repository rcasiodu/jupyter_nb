{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pickle, gzip\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST DataSet\n",
    "([mnist.pkl.gz](http://deeplearning.net/data/mnist/mnist.pkl.gz))\n",
    "\n",
    "The MNIST dataset consists of handwritten digit images and it is divided in 60,000 examples for the training set and 10,000 examples for testing. In many papers as well as in this tutorial, the official training set of 60,000 is divided into an actual training set of 50,000 examples and 10,000 validation examples (for selecting hyper-parameters like learning rate and size of the model). All digit images have been size-normalized and centered in a fixed size image of 28 x 28 pixels. In the original dataset each pixel of the image is represented by a value between 0 and 255, where 0 is black, 255 is white and anything in between is a different shade of grey.\n",
    "\n",
    "Here are some examples of MNIST digits:\n",
    "<div align=\"center\">\n",
    "<a><img style=\"float:left;\" alt=\"0\" src=\"http://www.deeplearning.net/tutorial/_images/mnist_0.png\" ></a>\n",
    "<a><img style=\"float:left;\" alt=\"1\" src=\"http://www.deeplearning.net/tutorial/_images/mnist_1.png\" ></a>\n",
    "<a><img style=\"float:left;\" alt=\"2\" src=\"http://www.deeplearning.net/tutorial/_images/mnist_2.png\" ></a>\n",
    "<a><img style=\"float:left;\" alt=\"3\" src=\"http://www.deeplearning.net/tutorial/_images/mnist_3.png\" ></a>\n",
    "<a><img style=\"float:left;\" alt=\"4\" src=\"http://www.deeplearning.net/tutorial/_images/mnist_4.png\" ></a>\n",
    "<a><img style=\"float:left;\" alt=\"5\" src=\"http://www.deeplearning.net/tutorial/_images/mnist_5.png\"></a>\n",
    "</div>\n",
    "<br />\n",
    "\n",
    "For convenience we pickled the dataset to make it easier to use in python. The pickled file represents a tuple of 3 lists : the training set, the validation set and the testing set. Each of the three lists is a pair formed from a list of images and a list of class labels for each of the images. An image is represented as numpy 1-dimensional array of 784 (28 x 28) float values between 0 and 1 (0 stands for black, 1 for white). The labels are numbers between 0 and 9 indicating which digit the image represents. The code block below shows how to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Network(object):\n",
    "        # initialize the weights and biases in nerual network with\n",
    "        #  normal distribution of mean=0 and std=1.0 by np.random.randn \n",
    "        def __init__(self, sizes):\n",
    "            self.num_layers = len(sizes) #including input and output layer\n",
    "            self.sizes = sizes\n",
    "            self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "            self.weights = [np.random.randn(y, x) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "            \n",
    "        # input a need to (n, 1) shape numpy ndarray\n",
    "        def feedforward(self, a):\n",
    "            \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "            for i in range(self.num_layers-1):\n",
    "                a = sigmoid(np.dot(self.weights[i], a)+ self.biases[i])  \n",
    "            \n",
    "            return a\n",
    "        \n",
    "        def SGD(self, training_data, epochs, mini_batch_size, eta,test_data=None):\n",
    "            \"\"\"Train the neural network using mini-batch stochastic\n",
    "                gradient descent. The \"training_data\" is a list of tuples\n",
    "                \"(x, y)\" representing the training inputs and the desired\n",
    "                outputs. The other non-optional parameters are\n",
    "                self-explanatory. If \"test_data\" is provided then the\n",
    "                network will be evaluated against the test data after each\n",
    "                epoch, and partial progress printed out. This is useful for\n",
    "                tracking progress, but slows things down substantially.\"\"\"\n",
    "            if test_data: \n",
    "                n_test = len(test_data)\n",
    "            n = len(training_data)\n",
    "            for j in range(epochs):\n",
    "                # random.shuffle cannot applied to np.array\n",
    "                random.shuffle(training_data)\n",
    "                mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "                for mini_batch in mini_batches:\n",
    "                    self.update_mini_batch(mini_batch, eta)\n",
    "                    \n",
    "                if test_data:\n",
    "                    print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "                else:\n",
    "                    print(\"Epoch {0} complete\".format(j))\n",
    "        \n",
    "        def update_mini_batch(self, mini_batch, eta):\n",
    "            \"\"\"Update the network's weights and biases by applying\n",
    "                gradient descent using backpropagation to a single mini batch.\n",
    "                The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"\n",
    "                is the learning rate.\"\"\"\n",
    "            nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "            nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "            for x, y in mini_batch:\n",
    "                delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "                nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "                nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "                self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "                self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "        def backprop(self, x, y):\n",
    "            \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "                gradient for the cost function C_x. ``nabla_b`` and\n",
    "                ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "                to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "            nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "            nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "            \n",
    "            # feedforward\n",
    "            activation = x\n",
    "            activations = [x] # list to store all the activations, layer by layer\n",
    "            zs = [] # list to store all the z vectors, layer by layer\n",
    "\n",
    "            for b, w in zip(self.biases, self.weights):\n",
    "                z = np.dot(w, activation)+b\n",
    "                zs.append(z)\n",
    "                activation = sigmoid(z)\n",
    "                activations.append(activation)\n",
    "            \n",
    "            # backward pass\n",
    "            delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "            nabla_b[-1] = delta\n",
    "            nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "            # Note that the variable l in the loop below is used a little\n",
    "            # differently to the notation in Chapter 2 of the book. Here,\n",
    "            # l = 1 means the last layer of neurons, l = 2 is the\n",
    "            # second-last layer, and so on. It's a renumbering of the\n",
    "            # scheme in the book, used here to take advantage of the fact\n",
    "            # that Python can use negative indices in lists.\n",
    "            for l in range(2, self.num_layers):\n",
    "                z = zs[-l]\n",
    "                sp = sigmoid_prime(z)\n",
    "                delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "                nabla_b[-l] = delta\n",
    "                nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            \n",
    "            return (nabla_b, nabla_w)\n",
    "        \n",
    "        def evaluate(self, test_data):\n",
    "            \"\"\"Return the number of test inputs for which the neural\n",
    "                network outputs the correct result. Note that the neural\n",
    "                network's output is assumed to be the index of whichever\n",
    "                neuron in the final layer has the highest activation.\"\"\"\n",
    "            test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "            return sum(int(x == y) for (x, y) in test_results)\n",
    "        \n",
    "        def cost_derivative(self, output_activations, y):\n",
    "            \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "                \\partial a for the output activations.\"\"\"\n",
    "            return (output_activations-y)\n",
    "            \n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f, encoding='iso-8859-1')\n",
    "f.close()\n",
    "\n",
    "# data set are tuple type with data[0]=img, data[1]=classification\n",
    "# data[0] and data[1] are numpy array, the shape of data[0] is (784,)\n",
    "# train_set = 50,000, valid_set = 10,000, test_set = 10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since now the data is in one variable, and a minibatch is defined as a slice of that variable, it comes more natural to define a minibatch by indicating its index and its size. In our setup the batch size stays constant throughout the execution of the code, therefore a function will actually require only the index to identify on which datapoints to work. The code below shows how to store your data and how to access a minibatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_wrapper(data, vectorize=False):\n",
    "    input_ = [x.reshape(784, 1) for x in data[0]]\n",
    "    #output_ =[vectorized_result(y) for y in data[1]]\n",
    "    if not vectorize:\n",
    "        output_ = data[1]\n",
    "    else:\n",
    "        output_ = [vectorized_result(y) for y in data[1]]\n",
    "    wrapper_data = list(zip(input_, output_))\n",
    "    return wrapper_data\n",
    "    \n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data warapper\n",
    "training_data = load_data_wrapper(train_set, True)\n",
    "validation_data = load_data_wrapper(valid_set, False)\n",
    "test_data = load_data_wrapper(test_set, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAHECAYAAAApqdEmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8810208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train =np.array([i[0].reshape(28, 28) for i in training_data])\n",
    "y_train = np.array([np.argmax(i[1]) for i in training_data])\n",
    "\n",
    "plt.figure('fig1', figsize=(6,8))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 8452 / 10000\n",
      "Epoch 1: 8652 / 10000\n",
      "Epoch 2: 8895 / 10000\n",
      "Epoch 3: 9056 / 10000\n",
      "Epoch 4: 8945 / 10000\n",
      "Epoch 5: 8976 / 10000\n",
      "Epoch 6: 9081 / 10000\n",
      "Epoch 7: 9065 / 10000\n",
      "Epoch 8: 8951 / 10000\n",
      "Epoch 9: 9103 / 10000\n",
      "Epoch 10: 9098 / 10000\n",
      "Epoch 11: 8903 / 10000\n",
      "Epoch 12: 9092 / 10000\n",
      "Epoch 13: 9124 / 10000\n",
      "Epoch 14: 9187 / 10000\n",
      "Epoch 15: 9181 / 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:109: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 9145 / 10000\n",
      "Epoch 17: 9103 / 10000\n",
      "Epoch 18: 9034 / 10000\n",
      "Epoch 19: 9148 / 10000\n",
      "Epoch 20: 9086 / 10000\n",
      "Epoch 21: 9170 / 10000\n",
      "Epoch 22: 9220 / 10000\n",
      "Epoch 23: 9241 / 10000\n",
      "Epoch 24: 9153 / 10000\n",
      "Epoch 25: 9243 / 10000\n",
      "Epoch 26: 9239 / 10000\n",
      "Epoch 27: 9282 / 10000\n",
      "Epoch 28: 9249 / 10000\n",
      "Epoch 29: 9296 / 10000\n"
     ]
    }
   ],
   "source": [
    "net = Network([784, 30, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 798 / 10000\n",
      "Epoch 1: 968 / 10000\n",
      "Epoch 2: 1216 / 10000\n",
      "Epoch 3: 2372 / 10000\n",
      "Epoch 4: 2160 / 10000\n",
      "Epoch 5: 1758 / 10000\n",
      "Epoch 6: 2215 / 10000\n",
      "Epoch 7: 2651 / 10000\n",
      "Epoch 8: 3303 / 10000\n",
      "Epoch 9: 2938 / 10000\n",
      "Epoch 10: 2776 / 10000\n",
      "Epoch 11: 2521 / 10000\n",
      "Epoch 12: 2987 / 10000\n",
      "Epoch 13: 2921 / 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:109: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 3012 / 10000\n",
      "Epoch 15: 3035 / 10000\n",
      "Epoch 16: 3060 / 10000\n",
      "Epoch 17: 2888 / 10000\n",
      "Epoch 18: 3290 / 10000\n",
      "Epoch 19: 3099 / 10000\n",
      "Epoch 20: 3006 / 10000\n",
      "Epoch 21: 3236 / 10000\n",
      "Epoch 22: 3094 / 10000\n",
      "Epoch 23: 2875 / 10000\n",
      "Epoch 24: 3261 / 10000\n",
      "Epoch 25: 3016 / 10000\n",
      "Epoch 26: 3246 / 10000\n",
      "Epoch 27: 3217 / 10000\n",
      "Epoch 28: 3302 / 10000\n",
      "Epoch 29: 3349 / 10000\n",
      "Epoch 30: 3373 / 10000\n",
      "Epoch 31: 3318 / 10000\n",
      "Epoch 32: 3445 / 10000\n",
      "Epoch 33: 3441 / 10000\n",
      "Epoch 34: 3526 / 10000\n",
      "Epoch 35: 3330 / 10000\n",
      "Epoch 36: 3374 / 10000\n",
      "Epoch 37: 3239 / 10000\n",
      "Epoch 38: 3347 / 10000\n",
      "Epoch 39: 3748 / 10000\n",
      "Epoch 40: 3609 / 10000\n",
      "Epoch 41: 3613 / 10000\n",
      "Epoch 42: 3446 / 10000\n",
      "Epoch 43: 3661 / 10000\n",
      "Epoch 44: 3760 / 10000\n",
      "Epoch 45: 3506 / 10000\n",
      "Epoch 46: 3477 / 10000\n",
      "Epoch 47: 3345 / 10000\n",
      "Epoch 48: 3436 / 10000\n",
      "Epoch 49: 3221 / 10000\n"
     ]
    }
   ],
   "source": [
    "net = Network([784, 100, 10])\n",
    "net.SGD(training_data, 50, 10, 5.0, test_data=test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
